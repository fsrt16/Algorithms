{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Selection using Pyswarms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRKCS2iYQdllhwkFNGfq6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsrt16/Algorithms/blob/master/Feature_Selection_using_Pyswarms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYWqZ_7ekT1C",
        "outputId": "dc0fb1ee-cb8c-45af-c2ed-54d543e65287"
      },
      "source": [
        "!pip install pyswarms"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyswarms\n",
            "  Downloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyswarms) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyswarms) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pyswarms) (4.62.3)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from pyswarms) (3.2.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from pyswarms) (21.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyswarms) (0.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from pyswarms) (3.13)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3.1->pyswarms) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3.1->pyswarms) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3.1->pyswarms) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.3.1->pyswarms) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.3.1->pyswarms) (1.15.0)\n",
            "Installing collected packages: pyswarms\n",
            "Successfully installed pyswarms-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jad2zviPkJRm"
      },
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Import PySwarms\n",
        "import pyswarms as ps\n",
        "\n",
        "# Some more magic so that the notebook will reload external python modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sA2yJhrkM_E"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=100, n_features=15, n_classes=3,\n",
        "                           n_informative=4, n_redundant=1, n_repeated=2,\n",
        "                           random_state=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "YHeClCPMkQHB",
        "outputId": "3c83480d-f047-41ee-e0b4-cc8be4ff1002"
      },
      "source": [
        "# Plot toy dataset per feature\n",
        "df = pd.DataFrame(X)\n",
        "df['labels'] = pd.Series(y)\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.655084</td>\n",
              "      <td>-2.128963</td>\n",
              "      <td>-0.685016</td>\n",
              "      <td>1.922747</td>\n",
              "      <td>0.645288</td>\n",
              "      <td>-2.224576</td>\n",
              "      <td>1.198653</td>\n",
              "      <td>-1.442570</td>\n",
              "      <td>0.225801</td>\n",
              "      <td>1.922747</td>\n",
              "      <td>2.454673</td>\n",
              "      <td>1.198653</td>\n",
              "      <td>0.884753</td>\n",
              "      <td>-0.601173</td>\n",
              "      <td>0.918193</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.303401</td>\n",
              "      <td>-1.378328</td>\n",
              "      <td>-0.566002</td>\n",
              "      <td>0.114298</td>\n",
              "      <td>-0.239843</td>\n",
              "      <td>-0.310322</td>\n",
              "      <td>1.501327</td>\n",
              "      <td>-0.044018</td>\n",
              "      <td>-0.472958</td>\n",
              "      <td>0.114298</td>\n",
              "      <td>0.492101</td>\n",
              "      <td>1.501327</td>\n",
              "      <td>-0.494118</td>\n",
              "      <td>-0.512464</td>\n",
              "      <td>0.141817</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.724706</td>\n",
              "      <td>-0.986521</td>\n",
              "      <td>0.389249</td>\n",
              "      <td>-1.895799</td>\n",
              "      <td>0.042914</td>\n",
              "      <td>0.472371</td>\n",
              "      <td>-0.318019</td>\n",
              "      <td>0.951178</td>\n",
              "      <td>0.644809</td>\n",
              "      <td>-1.895799</td>\n",
              "      <td>0.418651</td>\n",
              "      <td>-0.318019</td>\n",
              "      <td>-0.271892</td>\n",
              "      <td>-2.731398</td>\n",
              "      <td>0.936142</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.049146</td>\n",
              "      <td>-0.588923</td>\n",
              "      <td>0.326328</td>\n",
              "      <td>0.922614</td>\n",
              "      <td>1.131195</td>\n",
              "      <td>-0.983339</td>\n",
              "      <td>0.549084</td>\n",
              "      <td>1.621353</td>\n",
              "      <td>1.479451</td>\n",
              "      <td>0.922614</td>\n",
              "      <td>-0.951512</td>\n",
              "      <td>0.549084</td>\n",
              "      <td>-0.470488</td>\n",
              "      <td>0.787868</td>\n",
              "      <td>-0.696832</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.378306</td>\n",
              "      <td>-1.327988</td>\n",
              "      <td>0.647906</td>\n",
              "      <td>0.877558</td>\n",
              "      <td>-1.091076</td>\n",
              "      <td>-1.513970</td>\n",
              "      <td>1.208688</td>\n",
              "      <td>-0.459604</td>\n",
              "      <td>1.786182</td>\n",
              "      <td>0.877558</td>\n",
              "      <td>-1.650467</td>\n",
              "      <td>1.208688</td>\n",
              "      <td>-0.393068</td>\n",
              "      <td>1.295161</td>\n",
              "      <td>-0.512984</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>-0.177226</td>\n",
              "      <td>2.043693</td>\n",
              "      <td>1.136525</td>\n",
              "      <td>0.267609</td>\n",
              "      <td>-0.840022</td>\n",
              "      <td>2.573524</td>\n",
              "      <td>-0.365685</td>\n",
              "      <td>-1.487846</td>\n",
              "      <td>-0.479539</td>\n",
              "      <td>0.267609</td>\n",
              "      <td>-0.702859</td>\n",
              "      <td>-0.365685</td>\n",
              "      <td>1.137251</td>\n",
              "      <td>-1.456116</td>\n",
              "      <td>0.617562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.540818</td>\n",
              "      <td>-0.568785</td>\n",
              "      <td>0.470516</td>\n",
              "      <td>2.667254</td>\n",
              "      <td>-0.087918</td>\n",
              "      <td>-3.171549</td>\n",
              "      <td>-1.952123</td>\n",
              "      <td>-1.275209</td>\n",
              "      <td>-0.458240</td>\n",
              "      <td>2.667254</td>\n",
              "      <td>1.099393</td>\n",
              "      <td>-1.952123</td>\n",
              "      <td>-0.022824</td>\n",
              "      <td>-0.656279</td>\n",
              "      <td>-1.181429</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.671385</td>\n",
              "      <td>-0.336407</td>\n",
              "      <td>0.563321</td>\n",
              "      <td>1.780646</td>\n",
              "      <td>0.784192</td>\n",
              "      <td>-0.075812</td>\n",
              "      <td>0.302305</td>\n",
              "      <td>0.317170</td>\n",
              "      <td>-0.217989</td>\n",
              "      <td>1.780646</td>\n",
              "      <td>-0.837611</td>\n",
              "      <td>0.302305</td>\n",
              "      <td>0.572541</td>\n",
              "      <td>-2.339250</td>\n",
              "      <td>-0.068524</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1.917319</td>\n",
              "      <td>-0.306018</td>\n",
              "      <td>1.849917</td>\n",
              "      <td>-0.225949</td>\n",
              "      <td>0.609191</td>\n",
              "      <td>1.034543</td>\n",
              "      <td>1.791212</td>\n",
              "      <td>0.073941</td>\n",
              "      <td>0.314464</td>\n",
              "      <td>-0.225949</td>\n",
              "      <td>-0.253615</td>\n",
              "      <td>1.791212</td>\n",
              "      <td>0.297657</td>\n",
              "      <td>0.307610</td>\n",
              "      <td>-0.000731</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1.477501</td>\n",
              "      <td>1.627130</td>\n",
              "      <td>-1.234006</td>\n",
              "      <td>1.430260</td>\n",
              "      <td>-0.636191</td>\n",
              "      <td>0.975214</td>\n",
              "      <td>-0.163108</td>\n",
              "      <td>-0.637353</td>\n",
              "      <td>-0.297395</td>\n",
              "      <td>1.430260</td>\n",
              "      <td>-0.049065</td>\n",
              "      <td>-0.163108</td>\n",
              "      <td>-0.494582</td>\n",
              "      <td>0.482936</td>\n",
              "      <td>-2.009301</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2  ...        13        14  labels\n",
              "0   0.655084 -2.128963 -0.685016  ... -0.601173  0.918193       0\n",
              "1  -0.303401 -1.378328 -0.566002  ... -0.512464  0.141817       0\n",
              "2   0.724706 -0.986521  0.389249  ... -2.731398  0.936142       0\n",
              "3   0.049146 -0.588923  0.326328  ...  0.787868 -0.696832       2\n",
              "4   0.378306 -1.327988  0.647906  ...  1.295161 -0.512984       2\n",
              "..       ...       ...       ...  ...       ...       ...     ...\n",
              "95 -0.177226  2.043693  1.136525  ... -1.456116  0.617562       2\n",
              "96  0.540818 -0.568785  0.470516  ... -0.656279 -1.181429       0\n",
              "97  0.671385 -0.336407  0.563321  ... -2.339250 -0.068524       0\n",
              "98  1.917319 -0.306018  1.849917  ...  0.307610 -0.000731       1\n",
              "99  1.477501  1.627130 -1.234006  ...  0.482936 -2.009301       2\n",
              "\n",
              "[100 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvqBhK5-klbG"
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "# Create an instance of the classifier\n",
        "classifier = linear_model.LogisticRegression()\n",
        "\n",
        "# Define objective function\n",
        "def f_per_particle(m, alpha):\n",
        "    \"\"\"Computes for the objective function per particle\n",
        "\n",
        "    Inputs\n",
        "    ------\n",
        "    m : numpy.ndarray\n",
        "        Binary mask that can be obtained from BinaryPSO, will\n",
        "        be used to mask features.\n",
        "    alpha: float (default is 0.5)\n",
        "        Constant weight for trading-off classifier performance\n",
        "        and number of features\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Computed objective function\n",
        "    \"\"\"\n",
        "    total_features = 15\n",
        "    # Get the subset of the features from the binary mask\n",
        "    if np.count_nonzero(m) == 0:\n",
        "        X_subset = X\n",
        "    else:\n",
        "        X_subset = X[:,m==1]\n",
        "    # Perform classification and store performance in P\n",
        "    classifier.fit(X_subset, y)\n",
        "    P = (classifier.predict(X_subset) == y).mean()\n",
        "    # Compute for the objective function\n",
        "    j = (alpha * (1.0 - P)\n",
        "        + (1.0 - alpha) * (1 - (X_subset.shape[1] / total_features)))\n",
        "\n",
        "    return j\n",
        "def f(x, alpha=0.88):\n",
        "    \"\"\"Higher-level method to do classification in the\n",
        "    whole swarm.\n",
        "\n",
        "    Inputs\n",
        "    ------\n",
        "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
        "        The swarm that will perform the search\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray of shape (n_particles, )\n",
        "        The computed loss for each particle\n",
        "    \"\"\"\n",
        "    n_particles = x.shape[0]\n",
        "    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]\n",
        "    return np.array(j)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EezgPFyVkq1N",
        "outputId": "79a51289-ee36-487c-aaf3-5dcc3d3bef3f"
      },
      "source": [
        "# Initialize swarm, arbitrary\n",
        "options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n",
        "\n",
        "# Call instance of PSO\n",
        "dimensions = 15 # dimensions should be the number of features   \n",
        "optimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=dimensions, options=options)\n",
        "\n",
        "# Perform optimization\n",
        "cost, pos = optimizer.optimize(f,  iters=1000, verbose=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-13 13:55:03,924 - pyswarms.discrete.binary - INFO - Optimize for 1000 iters with {'c1': 0.5, 'c2': 0.5, 'w': 0.9, 'k': 30, 'p': 2}\n",
            "pyswarms.discrete.binary: 100%|██████████|1000/1000, best_cost=0.193\n",
            "2021-11-13 13:59:03,037 - pyswarms.discrete.binary - INFO - Optimization finished | best cost: 0.19279999999999997, best pos: [1 1 1 1 1 1 1 1 1 1 0 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwUD8Ky5la0t"
      },
      "source": [
        "# Create two instances of LogisticRegression\n",
        "classfier = linear_model.LogisticRegression()\n",
        "\n",
        "# Get the selected features from the final positions\n",
        "X_selected_features = X[:,pos==1]  # subset\n",
        "\n",
        "# Perform classification and store performance in P\n",
        "classifier.fit(X_selected_features, y)\n",
        "y_pred = classifier.predict(X_selected_features)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlKn3NWbmvi7",
        "outputId": "15a41e2b-f7fb-4956-a002-764930e388d0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y, y_pred))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83        34\n",
            "           1       0.84      0.81      0.83        32\n",
            "           2       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.79       100\n",
            "   macro avg       0.79      0.79      0.79       100\n",
            "weighted avg       0.79      0.79      0.79       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gl1F-5SnEm3"
      },
      "source": [
        "# Create two instances of LogisticRegression\n",
        "classfier = linear_model.LogisticRegression()\n",
        "\n",
        "\n",
        "# Perform classification and store performance in P\n",
        "classifier.fit(X, y)\n",
        "y_pred = classifier.predict(X)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tll6-_w7nLvk",
        "outputId": "c8017fd8-f8ea-4572-b7d5-de9cf122bd35"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.79      0.77        34\n",
            "           1       0.84      0.81      0.83        32\n",
            "           2       0.67      0.65      0.66        34\n",
            "\n",
            "    accuracy                           0.75       100\n",
            "   macro avg       0.75      0.75      0.75       100\n",
            "weighted avg       0.75      0.75      0.75       100\n",
            "\n"
          ]
        }
      ]
    }
  ]
}